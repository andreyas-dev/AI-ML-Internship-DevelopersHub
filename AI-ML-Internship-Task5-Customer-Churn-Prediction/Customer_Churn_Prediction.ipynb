{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53bd7f4",
   "metadata": {},
   "source": [
    "# üéØ Advanced Customer Churn Prediction System\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This comprehensive machine learning project analyzes customer behavior patterns to predict churn likelihood using advanced scikit-learn pipelines and hyperparameter optimization techniques.\n",
    "\n",
    "### üîç Key Features:\n",
    "\n",
    "- **End-to-end ML Pipeline**: Complete data preprocessing, model training, and evaluation workflow\n",
    "- **Advanced Feature Engineering**: Automated handling of mixed data types (numerical & categorical)\n",
    "- **Model Comparison**: Performance analysis between Logistic Regression and Random Forest algorithms\n",
    "- **Hyperparameter Optimization**: Grid search for optimal model configuration\n",
    "- **Interactive Deployment**: Streamlit web application for real-time predictions\n",
    "- **Production-Ready**: Serialized model pipeline for deployment scenarios\n",
    "\n",
    "### üìä Dataset: Telco Customer Churn\n",
    "\n",
    "- **Source**: IBM Telco Customer Dataset\n",
    "- **Objective**: Predict whether a customer will churn (cancel service)\n",
    "- **Features**: Demographics, services, account information, and charges\n",
    "- **Target**: Binary classification (Churn: Yes/No)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd5a95",
   "metadata": {},
   "source": [
    "## üìö Step 1: Environment Setup & Library Configuration\n",
    "\n",
    "Setting up our comprehensive machine learning toolkit with enhanced visualization capabilities and advanced preprocessing utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df642ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE MACHINE LEARNING TOOLKIT SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Core Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Advanced Visualization Suite\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Scikit-learn Complete Toolkit\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    RobustScaler,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Advanced Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Comprehensive Model Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# Model Persistence & Deployment Tools\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# System & Warning Management\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure environment for optimal performance\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "print(\"üöÄ ML Toolkit Successfully Loaded!\")\n",
    "print(f\"üìÖ Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64990f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REPRODUCIBILITY & RANDOMIZATION CONTROL CENTER\n",
    "# =============================================================================\n",
    "\n",
    "# Set master random seed for complete reproducibility across all libraries\n",
    "MASTER_RANDOM_STATE = (\n",
    "    42  # Answer to the Ultimate Question of Life, Universe, and Everything\n",
    ")\n",
    "ANALYSIS_VERSION = \"v2.1\"\n",
    "\n",
    "# Configure numpy random state\n",
    "np.random.seed(MASTER_RANDOM_STATE)\n",
    "\n",
    "# Additional reproducibility measures\n",
    "import random\n",
    "\n",
    "random.seed(MASTER_RANDOM_STATE)\n",
    "\n",
    "# Set environment variables for additional reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(MASTER_RANDOM_STATE)\n",
    "\n",
    "print(f\"üé≤ Reproducibility Configuration Complete!\")\n",
    "print(f\"üî¢ Master Random State: {MASTER_RANDOM_STATE}\")\n",
    "print(f\"üìã Analysis Version: {ANALYSIS_VERSION}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f15a1",
   "metadata": {},
   "source": [
    "## üì• Step 2: Advanced Data Acquisition & Exploratory Analysis\n",
    "\n",
    "Implementing robust data loading with comprehensive quality assessment and initial pattern discovery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b892ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7043, 21)\n",
      "\n",
      "First 5 rows:\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   object \n",
      " 1   gender            7043 non-null   object \n",
      " 2   SeniorCitizen     7043 non-null   int64  \n",
      " 3   Partner           7043 non-null   object \n",
      " 4   Dependents        7043 non-null   object \n",
      " 5   tenure            7043 non-null   int64  \n",
      " 6   PhoneService      7043 non-null   object \n",
      " 7   MultipleLines     7043 non-null   object \n",
      " 8   InternetService   7043 non-null   object \n",
      " 9   OnlineSecurity    7043 non-null   object \n",
      " 10  OnlineBackup      7043 non-null   object \n",
      " 11  DeviceProtection  7043 non-null   object \n",
      " 12  TechSupport       7043 non-null   object \n",
      " 13  StreamingTV       7043 non-null   object \n",
      " 14  StreamingMovies   7043 non-null   object \n",
      " 15  Contract          7043 non-null   object \n",
      " 16  PaperlessBilling  7043 non-null   object \n",
      " 17  PaymentMethod     7043 non-null   object \n",
      " 18  MonthlyCharges    7043 non-null   float64\n",
      " 19  TotalCharges      7043 non-null   object \n",
      " 20  Churn             7043 non-null   object \n",
      "dtypes: float64(1), int64(2), object(18)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "Missing values:\n",
      "customerID          0\n",
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "MultipleLines       0\n",
      "InternetService     0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "PaperlessBilling    0\n",
      "PaymentMethod       0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        0\n",
      "Churn               0\n",
      "dtype: int64\n",
      "\n",
      "Churn distribution:\n",
      "Churn\n",
      "No     5174\n",
      "Yes    1869\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Churn percentage:\n",
      "Churn\n",
      "No     0.73463\n",
      "Yes    0.26537\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Numerical features statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7043.000000</td>\n",
       "      <td>7043.000000</td>\n",
       "      <td>7043.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.162147</td>\n",
       "      <td>32.371149</td>\n",
       "      <td>64.761692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.368612</td>\n",
       "      <td>24.559481</td>\n",
       "      <td>30.090047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>35.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>70.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>89.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>118.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SeniorCitizen       tenure  MonthlyCharges\n",
       "count    7043.000000  7043.000000     7043.000000\n",
       "mean        0.162147    32.371149       64.761692\n",
       "std         0.368612    24.559481       30.090047\n",
       "min         0.000000     0.000000       18.250000\n",
       "25%         0.000000     9.000000       35.500000\n",
       "50%         0.000000    29.000000       70.350000\n",
       "75%         0.000000    55.000000       89.850000\n",
       "max         1.000000    72.000000      118.750000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED DATA ACQUISITION & COMPREHENSIVE ANALYSIS SUITE\n",
    "# =============================================================================\n",
    "\n",
    "# Define data source with robust error handling\n",
    "DATA_SOURCE_URL = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
    "BACKUP_DATA_PATH = \"telco_churn_backup.csv\"\n",
    "\n",
    "print(\"üîÑ Initiating Advanced Data Loading Process...\")\n",
    "\n",
    "try:\n",
    "    # Primary data acquisition attempt\n",
    "    telco_df = pd.read_csv(DATA_SOURCE_URL)\n",
    "    print(\"‚úÖ Data successfully loaded from primary source\")\n",
    "\n",
    "    # Save backup copy for future use\n",
    "    telco_df.to_csv(BACKUP_DATA_PATH, index=False)\n",
    "    print(f\"üíæ Backup copy saved as: {BACKUP_DATA_PATH}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Primary source failed: {e}\")\n",
    "    print(\"üîÑ Attempting backup data load...\")\n",
    "    telco_df = pd.read_csv(BACKUP_DATA_PATH)\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE DATA PROFILING DASHBOARD\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä TELCO CUSTOMER CHURN DATASET - COMPREHENSIVE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dataset Dimensions & Structure\n",
    "print(\n",
    "    f\"üìê Dataset Dimensions: {telco_df.shape[0]:,} rows √ó {telco_df.shape[1]} columns\"\n",
    ")\n",
    "print(f\"üíæ Memory Usage: {telco_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display sample records with enhanced formatting\n",
    "print(\"\\nüîç SAMPLE RECORDS PREVIEW:\")\n",
    "print(\"-\" * 50)\n",
    "display(telco_df.head(3).style.background_gradient(cmap=\"viridis\", alpha=0.3))\n",
    "\n",
    "# Advanced Data Quality Assessment\n",
    "print(\"\\nüî¨ DATA QUALITY ASSESSMENT:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_analysis = telco_df.isnull().sum()\n",
    "missing_percentage = (missing_analysis / len(telco_df)) * 100\n",
    "missing_report = pd.DataFrame(\n",
    "    {\"Missing_Count\": missing_analysis, \"Missing_Percentage\": missing_percentage}\n",
    ").round(2)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_report[missing_report[\"Missing_Count\"] > 0])\n",
    "\n",
    "# Data types analysis\n",
    "print(\"\\nüìã DATA TYPES DISTRIBUTION:\")\n",
    "dtype_summary = telco_df.dtypes.value_counts()\n",
    "for dtype, count in dtype_summary.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "# Target variable analysis with enhanced visualization\n",
    "print(\"\\nüéØ TARGET VARIABLE ANALYSIS (CHURN):\")\n",
    "print(\"-\" * 45)\n",
    "churn_distribution = telco_df[\"Churn\"].value_counts()\n",
    "churn_percentages = telco_df[\"Churn\"].value_counts(normalize=True) * 100\n",
    "\n",
    "churn_summary = pd.DataFrame(\n",
    "    {\"Count\": churn_distribution, \"Percentage\": churn_percentages.round(2)}\n",
    ")\n",
    "print(churn_summary)\n",
    "\n",
    "# Calculate churn rate\n",
    "churn_rate = (churn_distribution[\"Yes\"] / churn_distribution.sum()) * 100\n",
    "print(f\"\\nüìà Overall Churn Rate: {churn_rate:.2f}%\")\n",
    "\n",
    "# Quick statistical overview for numerical columns\n",
    "print(\"\\nüìä NUMERICAL FEATURES STATISTICAL SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "numerical_summary = telco_df.describe().round(2)\n",
    "display(numerical_summary.style.background_gradient(cmap=\"coolwarm\", alpha=0.5))\n",
    "\n",
    "print(f\"\\n‚ú® Data Loading & Analysis Complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bd5f5",
   "metadata": {},
   "source": [
    "## üîß Step 3: Advanced Data Engineering & Feature Preparation\n",
    "\n",
    "Implementing sophisticated data preprocessing techniques with intelligent feature engineering and robust train-test splitting strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c9c106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in TotalCharges: 11\n",
      "Training set shape: (5634, 19)\n",
      "Testing set shape: (1409, 19)\n",
      "Churn distribution in training set:\n",
      "Churn\n",
      "0    0.734647\n",
      "1    0.265353\n",
      "Name: proportion, dtype: float64\n",
      "Churn distribution in testing set:\n",
      "Churn\n",
      "0    0.734564\n",
      "1    0.265436\n",
      "Name: proportion, dtype: float64\n",
      "Categorical columns: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "Numerical columns: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED DATA ENGINEERING & FEATURE PREPARATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîß Initiating Advanced Data Engineering Pipeline...\")\n",
    "\n",
    "# Create working copy with enhanced naming convention\n",
    "customer_data = telco_df.copy()\n",
    "customer_data.name = \"Enhanced_Telco_Dataset_v2.1\"\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED DATA CLEANING & TRANSFORMATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 3.1: Advanced Data Cleaning\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Handle TotalCharges data type conversion with enhanced error handling\n",
    "print(\"üîÑ Processing TotalCharges column...\")\n",
    "original_dtype = customer_data[\"TotalCharges\"].dtype\n",
    "customer_data[\"TotalCharges\"] = pd.to_numeric(\n",
    "    customer_data[\"TotalCharges\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Analyze and handle missing values in TotalCharges\n",
    "total_charges_missing = customer_data[\"TotalCharges\"].isnull().sum()\n",
    "print(f\"  üìà Missing TotalCharges values: {total_charges_missing}\")\n",
    "\n",
    "if total_charges_missing > 0:\n",
    "    # Advanced imputation strategy: use median for customers with similar tenure\n",
    "    print(\"  üõ†Ô∏è  Applying intelligent imputation strategy...\")\n",
    "    customer_data[\"TotalCharges\"] = customer_data.groupby(\"tenure\")[\n",
    "        \"TotalCharges\"\n",
    "    ].transform(lambda x: x.fillna(x.median()))\n",
    "    # Fill any remaining nulls with overall median\n",
    "    customer_data[\"TotalCharges\"].fillna(\n",
    "        customer_data[\"TotalCharges\"].median(), inplace=True\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"  ‚úÖ TotalCharges conversion: {original_dtype} ‚Üí {customer_data['TotalCharges'].dtype}\"\n",
    ")\n",
    "\n",
    "# Enhanced feature engineering: Create additional derived features\n",
    "print(\"\\nüî¨ Step 3.2: Advanced Feature Engineering\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Calculate average monthly charges per tenure\n",
    "customer_data[\"AvgChargesPerMonth\"] = customer_data[\"TotalCharges\"] / (\n",
    "    customer_data[\"tenure\"] + 1\n",
    ")\n",
    "\n",
    "# Create customer value segments\n",
    "customer_data[\"CustomerValueSegment\"] = pd.cut(\n",
    "    customer_data[\"TotalCharges\"],\n",
    "    bins=4,\n",
    "    labels=[\"Low_Value\", \"Medium_Value\", \"High_Value\", \"Premium_Value\"],\n",
    ")\n",
    "\n",
    "# Create tenure segments\n",
    "customer_data[\"TenureSegment\"] = pd.cut(\n",
    "    customer_data[\"tenure\"],\n",
    "    bins=[0, 12, 24, 48, 100],\n",
    "    labels=[\"New_Customer\", \"Growing\", \"Mature\", \"Loyal\"],\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"  ‚ûï Added derived features: AvgChargesPerMonth, CustomerValueSegment, TenureSegment\"\n",
    ")\n",
    "\n",
    "# Remove identifier column with confirmation\n",
    "identifier_cols = [\"customerID\"]\n",
    "customer_data.drop(columns=identifier_cols, inplace=True, errors=\"ignore\")\n",
    "print(f\"  üóëÔ∏è  Removed identifier columns: {identifier_cols}\")\n",
    "\n",
    "# =============================================================================\n",
    "# INTELLIGENT TARGET VARIABLE PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüéØ Step 3.3: Target Variable Engineering\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Separate features and target with enhanced naming\n",
    "feature_matrix = customer_data.drop(\"Churn\", axis=1)\n",
    "target_vector = customer_data[\"Churn\"].copy()\n",
    "\n",
    "# Enhanced target encoding with clear mapping\n",
    "churn_mapping = {\"Yes\": 1, \"No\": 0}\n",
    "target_vector = target_vector.map(churn_mapping)\n",
    "\n",
    "print(f\"  üîÑ Target encoding applied: {churn_mapping}\")\n",
    "print(f\"  üìä Target distribution after encoding:\")\n",
    "print(f\"    No Churn (0): {(target_vector == 0).sum():,} samples\")\n",
    "print(f\"    Churn (1): {(target_vector == 1).sum():,} samples\")\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED STRATIFIED TRAIN-TEST SPLITTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 3.4: Stratified Dataset Partitioning\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Enhanced train-test split with stratification\n",
    "TEST_SIZE_RATIO = 0.25  # Increased test size for more robust evaluation\n",
    "VALIDATION_SIZE_RATIO = 0.15\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_matrix,\n",
    "    target_vector,\n",
    "    test_size=TEST_SIZE_RATIO,\n",
    "    random_state=MASTER_RANDOM_STATE,\n",
    "    stratify=target_vector,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"  üìê Training Set: {X_train.shape[0]:,} samples ({(len(X_train) / len(customer_data) * 100):.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  üìê Testing Set:  {X_test.shape[0]:,} samples ({(len(X_test) / len(customer_data) * 100):.1f}%)\"\n",
    ")\n",
    "\n",
    "# Verify stratification quality\n",
    "train_churn_rate = (y_train.sum() / len(y_train)) * 100\n",
    "test_churn_rate = (y_test.sum() / len(y_test)) * 100\n",
    "\n",
    "print(f\"\\n  üìà Stratification Quality Check:\")\n",
    "print(f\"    Training Churn Rate:   {train_churn_rate:.2f}%\")\n",
    "print(f\"    Testing Churn Rate:    {test_churn_rate:.2f}%\")\n",
    "print(f\"    Stratification Diff:   {abs(train_churn_rate - test_churn_rate):.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# INTELLIGENT FEATURE TYPE CLASSIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüîç Step 3.5: Advanced Feature Type Analysis\")\n",
    "print(\"-\" * 44)\n",
    "\n",
    "# Enhanced feature type detection\n",
    "numerical_features = X_train.select_dtypes(\n",
    "    include=[\"int64\", \"float64\"]\n",
    ").columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Remove derived categorical features from standard processing if needed\n",
    "derived_categorical = [\"CustomerValueSegment\", \"TenureSegment\"]\n",
    "standard_categorical = [\n",
    "    col for col in categorical_features if col not in derived_categorical\n",
    "]\n",
    "\n",
    "print(f\"  üî¢ Numerical Features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(\n",
    "    f\"  üè∑Ô∏è  Categorical Features ({len(categorical_features)}): {categorical_features}\"\n",
    ")\n",
    "print(f\"  ‚≠ê Derived Features ({len(derived_categorical)}): {derived_categorical}\")\n",
    "\n",
    "print(f\"\\n‚ú® Advanced Data Engineering Pipeline Complete!\")\n",
    "print(f\"üéØ Ready for ML Pipeline Construction\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e868a15",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 4: Advanced ML Pipeline Architecture & Model Factory\n",
    "\n",
    "Constructing sophisticated machine learning pipelines with intelligent preprocessing, advanced feature transformations, and ensemble model strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ece71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED MACHINE LEARNING PIPELINE ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üèóÔ∏è Constructing Advanced ML Pipeline Architecture...\")\n",
    "\n",
    "# =============================================================================\n",
    "# SOPHISTICATED PREPROCESSING TRANSFORMERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüî¨ Step 4.1: Advanced Preprocessing Transformer Design\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "# Enhanced numerical preprocessing pipeline with robust scaling\n",
    "numerical_preprocessing_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\",\n",
    "            KNNImputer(n_neighbors=5, weights=\"distance\"),\n",
    "        ),  # Advanced KNN imputation\n",
    "        (\"scaler\", RobustScaler()),  # Robust to outliers\n",
    "        (\"normalizer\", StandardScaler()),  # Final standardization\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Advanced categorical preprocessing with enhanced encoding strategies\n",
    "categorical_preprocessing_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\",\n",
    "            SimpleImputer(strategy=\"most_frequent\", fill_value=\"Unknown\"),\n",
    "        ),  # Intelligent missing value handling\n",
    "        (\n",
    "            \"encoder\",\n",
    "            OneHotEncoder(\n",
    "                handle_unknown=\"ignore\",\n",
    "                sparse_output=False,\n",
    "                drop=\"if_binary\",  # Optimize binary features\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"  ‚úÖ Numerical Pipeline: KNN Imputation ‚Üí Robust Scaling ‚Üí Standardization\")\n",
    "print(\"  ‚úÖ Categorical Pipeline: Mode Imputation ‚Üí One-Hot Encoding (Optimized)\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE COLUMN TRANSFORMER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüîß Step 4.2: Comprehensive Feature Transformation Matrix\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "# Advanced column transformer with named transformers\n",
    "advanced_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numerical_features\", numerical_preprocessing_pipeline, numerical_features),\n",
    "        (\n",
    "            \"categorical_features\",\n",
    "            categorical_preprocessing_pipeline,\n",
    "            categorical_features,\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",  # Keep any additional features\n",
    "    n_jobs=-1,  # Parallel processing\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "print(f\"  üî¢ Numerical features to process: {len(numerical_features)}\")\n",
    "print(f\"  üè∑Ô∏è  Categorical features to process: {len(categorical_features)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED MODEL FACTORY & PIPELINE CONSTRUCTOR\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüè≠ Step 4.3: Advanced ML Model Factory\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "def create_enhanced_ml_pipeline(model_instance, pipeline_name=\"Custom_Pipeline\"):\n",
    "    \"\"\"\n",
    "    Enhanced pipeline factory with comprehensive preprocessing and model integration\n",
    "\n",
    "    Args:\n",
    "        model_instance: Scikit-learn compatible model\n",
    "        pipeline_name: Descriptive name for the pipeline\n",
    "\n",
    "    Returns:\n",
    "        Complete ML pipeline with preprocessing and model\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"advanced_preprocessing\", advanced_preprocessor),\n",
    "            (\"classifier\", model_instance),\n",
    "        ],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Add pipeline metadata\n",
    "    pipeline.pipeline_name = pipeline_name\n",
    "    pipeline.creation_timestamp = datetime.now()\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE MODEL SUITE CONSTRUCTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nü§ñ Step 4.4: Multi-Algorithm Model Suite Construction\")\n",
    "print(\"-\" * 56)\n",
    "\n",
    "# Enhanced Logistic Regression with advanced configuration\n",
    "enhanced_logistic_model = create_enhanced_ml_pipeline(\n",
    "    LogisticRegression(\n",
    "        random_state=MASTER_RANDOM_STATE,\n",
    "        max_iter=2000,  # Increased iterations\n",
    "        solver=\"liblinear\",  # Better for small datasets\n",
    "        class_weight=\"balanced\",  # Handle class imbalance\n",
    "        penalty=\"l2\",\n",
    "    ),\n",
    "    pipeline_name=\"Enhanced_Logistic_Regression_v2.1\",\n",
    ")\n",
    "\n",
    "# Advanced Random Forest with optimized parameters\n",
    "enhanced_random_forest = create_enhanced_ml_pipeline(\n",
    "    RandomForestClassifier(\n",
    "        random_state=MASTER_RANDOM_STATE,\n",
    "        n_estimators=150,  # Increased base estimators\n",
    "        max_depth=15,  # Controlled depth\n",
    "        min_samples_split=10,  # Prevent overfitting\n",
    "        min_samples_leaf=5,\n",
    "        class_weight=\"balanced\",  # Handle class imbalance\n",
    "        n_jobs=-1,  # Parallel processing\n",
    "        oob_score=True,  # Out-of-bag evaluation\n",
    "    ),\n",
    "    pipeline_name=\"Enhanced_Random_Forest_v2.1\",\n",
    ")\n",
    "\n",
    "# Advanced Gradient Boosting Model (New Addition)\n",
    "enhanced_gradient_boosting = create_enhanced_ml_pipeline(\n",
    "    GradientBoostingClassifier(\n",
    "        random_state=MASTER_RANDOM_STATE,\n",
    "        n_estimators=120,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        tol=1e-4,\n",
    "    ),\n",
    "    pipeline_name=\"Enhanced_Gradient_Boosting_v2.1\",\n",
    ")\n",
    "\n",
    "# Model registry for organized management\n",
    "model_registry = {\n",
    "    \"Enhanced_Logistic_Regression\": enhanced_logistic_model,\n",
    "    \"Enhanced_Random_Forest\": enhanced_random_forest,\n",
    "    \"Enhanced_Gradient_Boosting\": enhanced_gradient_boosting,\n",
    "}\n",
    "\n",
    "print(\"  üéØ Model Suite Successfully Constructed:\")\n",
    "print(\"    ‚úÖ Enhanced Logistic Regression (Balanced)\")\n",
    "print(\"    ‚úÖ Enhanced Random Forest (OOB Scoring)\")\n",
    "print(\"    ‚úÖ Enhanced Gradient Boosting (Early Stopping)\")\n",
    "\n",
    "print(f\"\\nüèÜ Advanced ML Pipeline Architecture Complete!\")\n",
    "print(f\"üìä {len(model_registry)} Models Ready for Training\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334012da",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Comprehensive Model Training & Performance Evaluation\n",
    "\n",
    "Implementing advanced model training protocols with cross-validation, comprehensive metrics evaluation, and detailed performance analysis across multiple algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47b4bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8055358410220014\n",
      "ROC AUC: 0.8421349040274871\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87      1035\n",
      "           1       0.66      0.56      0.60       374\n",
      "\n",
      "    accuracy                           0.81      1409\n",
      "   macro avg       0.75      0.73      0.74      1409\n",
      "weighted avg       0.80      0.81      0.80      1409\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Results:\n",
      "Accuracy: 0.7863733144073811\n",
      "ROC AUC: 0.8185099072567104\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86      1035\n",
      "           1       0.63      0.49      0.55       374\n",
      "\n",
      "    accuracy                           0.79      1409\n",
      "   macro avg       0.73      0.69      0.70      1409\n",
      "weighted avg       0.77      0.79      0.78      1409\n",
      "\n",
      "Model Comparison:\n",
      "                 Model  Accuracy   ROC AUC\n",
      "0  Logistic Regression  0.805536  0.842135\n",
      "1        Random Forest  0.786373  0.818510\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE MODEL TRAINING & ADVANCED PERFORMANCE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéØ Initiating Comprehensive Model Training & Evaluation Protocol...\")\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED CROSS-VALIDATION SETUP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüîÑ Step 5.1: Advanced Cross-Validation Configuration\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "# Setup stratified k-fold cross-validation for robust evaluation\n",
    "cv_strategy = StratifiedKFold(\n",
    "    n_splits=5, shuffle=True, random_state=MASTER_RANDOM_STATE\n",
    ")\n",
    "scoring_metrics = [\"accuracy\", \"roc_auc\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "print(f\"  üìä CV Strategy: {cv_strategy.n_splits}-Fold Stratified Cross-Validation\")\n",
    "print(f\"  üìà Scoring Metrics: {scoring_metrics}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE MODEL TRAINING & EVALUATION ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def comprehensive_model_evaluation(\n",
    "    model_pipeline, model_name, X_train, X_test, y_train, y_test\n",
    "):\n",
    "    \"\"\"\n",
    "    Advanced model evaluation with comprehensive metrics and analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Training & Evaluating: {model_name}\")\n",
    "    print(\"-\" * (25 + len(model_name)))\n",
    "\n",
    "    # Training phase with timing\n",
    "    start_time = datetime.now()\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "    # Cross-validation evaluation\n",
    "    cv_scores = {}\n",
    "    for metric in scoring_metrics:\n",
    "        scores = cross_val_score(\n",
    "            model_pipeline, X_train, y_train, cv=cv_strategy, scoring=metric, n_jobs=-1\n",
    "        )\n",
    "        cv_scores[metric] = {\n",
    "            \"mean\": scores.mean(),\n",
    "            \"std\": scores.std(),\n",
    "            \"scores\": scores,\n",
    "        }\n",
    "\n",
    "    # Test set predictions\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    y_pred_proba = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Comprehensive metrics calculation\n",
    "    test_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    # Results reporting\n",
    "    print(f\"  ‚è±Ô∏è  Training Time: {training_time:.2f} seconds\")\n",
    "    print(f\"  üìä Cross-Validation Results:\")\n",
    "    for metric, values in cv_scores.items():\n",
    "        print(f\"    {metric.upper()}: {values['mean']:.4f} (¬±{values['std']:.4f})\")\n",
    "\n",
    "    print(f\"  üéØ Test Set Performance:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"    {metric.upper()}: {value:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_pipeline,\n",
    "        \"model_name\": model_name,\n",
    "        \"training_time\": training_time,\n",
    "        \"cv_scores\": cv_scores,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"predictions\": y_pred,\n",
    "        \"prediction_probabilities\": y_pred_proba,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE MODEL TRAINING EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüè≠ Step 5.2: Multi-Model Training & Evaluation Execution\")\n",
    "print(\"-\" * 56)\n",
    "\n",
    "# Store comprehensive evaluation results\n",
    "model_evaluation_results = {}\n",
    "\n",
    "# Evaluate each model in the registry\n",
    "for model_key, model_pipeline in model_registry.items():\n",
    "    evaluation_results = comprehensive_model_evaluation(\n",
    "        model_pipeline, model_key, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    model_evaluation_results[model_key] = evaluation_results\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED PERFORMANCE COMPARISON & ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìä Step 5.3: Advanced Multi-Model Performance Analysis\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "# Create comprehensive comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name, results in model_evaluation_results.items():\n",
    "    row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Training_Time_Sec\": results[\"training_time\"],\n",
    "        \"CV_Accuracy_Mean\": results[\"cv_scores\"][\"accuracy\"][\"mean\"],\n",
    "        \"CV_Accuracy_Std\": results[\"cv_scores\"][\"accuracy\"][\"std\"],\n",
    "        \"CV_ROC_AUC_Mean\": results[\"cv_scores\"][\"roc_auc\"][\"mean\"],\n",
    "        \"CV_ROC_AUC_Std\": results[\"cv_scores\"][\"roc_auc\"][\"std\"],\n",
    "        \"Test_Accuracy\": results[\"test_metrics\"][\"accuracy\"],\n",
    "        \"Test_ROC_AUC\": results[\"test_metrics\"][\"roc_auc\"],\n",
    "        \"Test_Precision\": results[\"test_metrics\"][\"precision\"],\n",
    "        \"Test_Recall\": results[\"test_metrics\"][\"recall\"],\n",
    "        \"Test_F1_Score\": results[\"test_metrics\"][\"f1\"],\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create and display comprehensive comparison\n",
    "model_comparison_df = pd.DataFrame(comparison_data)\n",
    "model_comparison_df = model_comparison_df.round(4)\n",
    "\n",
    "print(\"üèÜ COMPREHENSIVE MODEL PERFORMANCE LEADERBOARD:\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    model_comparison_df.style.background_gradient(\n",
    "        cmap=\"RdYlGn\", subset=[\"Test_ROC_AUC\", \"Test_F1_Score\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Identify best performing model\n",
    "best_model_idx = model_comparison_df[\"Test_ROC_AUC\"].idxmax()\n",
    "best_model_name = model_comparison_df.loc[best_model_idx, \"Model\"]\n",
    "best_model_auc = model_comparison_df.loc[best_model_idx, \"Test_ROC_AUC\"]\n",
    "\n",
    "print(f\"\\nü•á CHAMPION MODEL: {best_model_name}\")\n",
    "print(f\"üéØ Best ROC-AUC Score: {best_model_auc:.4f}\")\n",
    "\n",
    "# Detailed classification report for best model\n",
    "print(f\"\\nüìã DETAILED CLASSIFICATION REPORT - {best_model_name}:\")\n",
    "print(\"-\" * (45 + len(best_model_name)))\n",
    "best_predictions = model_evaluation_results[best_model_name][\"predictions\"]\n",
    "print(\n",
    "    classification_report(y_test, best_predictions, target_names=[\"No Churn\", \"Churn\"])\n",
    ")\n",
    "\n",
    "print(f\"\\n‚ú® Comprehensive Model Training & Evaluation Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4305d19",
   "metadata": {},
   "source": [
    "## ‚ö° Step 6: Advanced Hyperparameter Optimization & Model Fine-Tuning\n",
    "\n",
    "Implementing sophisticated hyperparameter optimization strategies using GridSearchCV with comprehensive parameter space exploration and performance optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c158e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED HYPERPARAMETER OPTIMIZATION & MODEL FINE-TUNING ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"‚ö° Initiating Advanced Hyperparameter Optimization Protocol...\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE HYPERPARAMETER GRID DESIGN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüî¨ Step 6.1: Advanced Parameter Space Definition\")\n",
    "print(\"-\" * 47)\n",
    "\n",
    "# Enhanced Random Forest hyperparameter grid with comprehensive parameter space\n",
    "enhanced_rf_param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 150, 200, 250],  # Extended range\n",
    "    \"classifier__max_depth\": [None, 10, 15, 20, 25],  # More depth options\n",
    "    \"classifier__min_samples_split\": [2, 5, 8, 10],  # Overfitting control\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 4, 6],  # Leaf size optimization\n",
    "    \"classifier__max_features\": [\"sqrt\", \"log2\", 0.8],  # Feature selection strategies\n",
    "    \"classifier__bootstrap\": [True],  # Bootstrap sampling\n",
    "    \"classifier__class_weight\": [\n",
    "        \"balanced\",\n",
    "        \"balanced_subsample\",\n",
    "    ],  # Imbalance handling\n",
    "}\n",
    "\n",
    "# Alternative parameter grid for faster optimization (reduced search space)\n",
    "quick_rf_param_grid = {\n",
    "    \"classifier__n_estimators\": [150, 200],\n",
    "    \"classifier__max_depth\": [15, 20],\n",
    "    \"classifier__min_samples_split\": [5, 8],\n",
    "    \"classifier__min_samples_leaf\": [2, 4],\n",
    "    \"classifier__max_features\": [\"sqrt\", 0.8],\n",
    "}\n",
    "\n",
    "# Select parameter grid based on computational requirements\n",
    "SELECTED_PARAM_GRID = (\n",
    "    enhanced_rf_param_grid  # Change to quick_rf_param_grid for faster execution\n",
    ")\n",
    "GRID_SIZE = (\n",
    "    len(SELECTED_PARAM_GRID[\"classifier__n_estimators\"])\n",
    "    * len(SELECTED_PARAM_GRID[\"classifier__max_depth\"])\n",
    "    * len(SELECTED_PARAM_GRID[\"classifier__min_samples_split\"])\n",
    "    * len(SELECTED_PARAM_GRID[\"classifier__min_samples_leaf\"])\n",
    ")\n",
    "\n",
    "print(f\"  üìä Parameter Grid Size: {GRID_SIZE:,} combinations\")\n",
    "print(f\"  üéØ Optimization Target: ROC-AUC Score\")\n",
    "print(\n",
    "    f\"  ‚öôÔ∏è  Selected Grid: {'Enhanced' if SELECTED_PARAM_GRID == enhanced_rf_param_grid else 'Quick'}\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED GRID SEARCH CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüîç Step 6.2: Advanced Grid Search Configuration\")\n",
    "print(\"-\" * 46)\n",
    "\n",
    "# Enhanced GridSearchCV with comprehensive configuration\n",
    "advanced_grid_search = GridSearchCV(\n",
    "    estimator=enhanced_random_forest,\n",
    "    param_grid=SELECTED_PARAM_GRID,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=MASTER_RANDOM_STATE),\n",
    "    scoring=\"roc_auc\",  # Primary optimization metric\n",
    "    n_jobs=-1,  # Utilize all available cores\n",
    "    verbose=2,  # Enhanced verbosity\n",
    "    return_train_score=True,  # Track training scores\n",
    "    error_score=\"raise\",  # Raise errors for debugging\n",
    ")\n",
    "\n",
    "print(f\"  üîÑ Cross-Validation: 5-Fold Stratified\")\n",
    "print(f\"  üíª Parallel Processing: All available cores\")\n",
    "print(f\"  üìà Scoring Metric: ROC-AUC\")\n",
    "print(f\"  ‚è±Ô∏è  Estimated Runtime: {(GRID_SIZE * 5 * 0.5):.1f} seconds\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETER OPTIMIZATION EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüöÄ Step 6.3: Executing Advanced Hyperparameter Optimization\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Execute grid search with comprehensive timing\n",
    "optimization_start_time = datetime.now()\n",
    "print(f\"  üïê Optimization Started: {optimization_start_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"  üîÑ Processing {GRID_SIZE:,} parameter combinations...\")\n",
    "\n",
    "# Perform comprehensive grid search\n",
    "advanced_grid_search.fit(X_train, y_train)\n",
    "\n",
    "optimization_end_time = datetime.now()\n",
    "optimization_duration = (\n",
    "    optimization_end_time - optimization_start_time\n",
    ").total_seconds()\n",
    "\n",
    "print(f\"  ‚úÖ Optimization Completed: {optimization_end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"  ‚è±Ô∏è  Total Duration: {optimization_duration:.2f} seconds\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZATION RESULTS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüèÜ Step 6.4: Optimization Results Analysis\")\n",
    "print(\"-\" * 41)\n",
    "\n",
    "# Extract optimal model and parameters\n",
    "champion_model = advanced_grid_search.best_estimator_\n",
    "optimal_parameters = advanced_grid_search.best_params_\n",
    "optimal_cv_score = advanced_grid_search.best_score_\n",
    "\n",
    "print(f\"  ü•á Optimal Cross-Validation Score: {optimal_cv_score:.6f}\")\n",
    "print(f\"  ‚öôÔ∏è  Optimal Parameters:\")\n",
    "for param, value in optimal_parameters.items():\n",
    "    param_clean = param.replace(\"classifier__\", \"\")\n",
    "    print(f\"    {param_clean}: {value}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CHAMPION MODEL COMPREHENSIVE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüéØ Step 6.5: Champion Model Performance Evaluation\")\n",
    "print(\"-\" * 49)\n",
    "\n",
    "# Generate predictions with champion model\n",
    "champion_predictions = champion_model.predict(X_test)\n",
    "champion_pred_probabilities = champion_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Comprehensive performance metrics\n",
    "champion_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, champion_predictions),\n",
    "    \"roc_auc\": roc_auc_score(y_test, champion_pred_probabilities),\n",
    "    \"precision\": precision_score(y_test, champion_predictions),\n",
    "    \"recall\": recall_score(y_test, champion_predictions),\n",
    "    \"f1_score\": f1_score(y_test, champion_predictions),\n",
    "}\n",
    "\n",
    "print(f\"  üéØ CHAMPION MODEL PERFORMANCE METRICS:\")\n",
    "print(f\"  \" + \"=\" * 40)\n",
    "for metric, value in champion_metrics.items():\n",
    "    print(f\"    {metric.upper()}: {value:.6f}\")\n",
    "\n",
    "# Performance improvement analysis\n",
    "baseline_auc = model_evaluation_results[\"Enhanced_Random_Forest\"][\"test_metrics\"][\n",
    "    \"roc_auc\"\n",
    "]\n",
    "improvement = ((champion_metrics[\"roc_auc\"] - baseline_auc) / baseline_auc) * 100\n",
    "\n",
    "print(f\"\\n  üìà Performance Improvement Analysis:\")\n",
    "print(f\"    Baseline Random Forest AUC: {baseline_auc:.6f}\")\n",
    "print(f\"    Champion Model AUC: {champion_metrics['roc_auc']:.6f}\")\n",
    "print(f\"    Relative Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n  üìã CHAMPION MODEL CLASSIFICATION REPORT:\")\n",
    "print(f\"  \" + \"-\" * 50)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, champion_predictions, target_names=[\"No Churn\", \"Churn\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\n‚ú® Advanced Hyperparameter Optimization Complete!\")\n",
    "print(f\"üèÜ Champion Model Ready for Deployment\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c1efa",
   "metadata": {},
   "source": [
    "## üìä Step 7: Advanced Model Visualization & Performance Analytics\n",
    "\n",
    "Creating comprehensive visualizations including confusion matrices, ROC curves, feature importance analysis, and advanced performance dashboards for thorough model interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED MODEL VISUALIZATION & COMPREHENSIVE PERFORMANCE ANALYTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Generating Advanced Visualization & Performance Analytics Suite...\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED CONFUSION MATRIX VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüéØ Step 7.1: Advanced Confusion Matrix Analysis\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Generate enhanced confusion matrix\n",
    "cm_champion = confusion_matrix(y_test, champion_predictions)\n",
    "cm_normalized = confusion_matrix(y_test, champion_predictions, normalize=\"true\")\n",
    "\n",
    "# Create comprehensive confusion matrix visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Raw counts confusion matrix\n",
    "sns.heatmap(\n",
    "    cm_champion,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"No Churn\", \"Churn\"],\n",
    "    yticklabels=[\"No Churn\", \"Churn\"],\n",
    "    ax=axes[0],\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "axes[0].set_title(\n",
    "    \"Champion Model - Confusion Matrix (Raw Counts)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].set_ylabel(\"Actual Labels\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Predicted Labels\", fontweight=\"bold\")\n",
    "\n",
    "# Normalized confusion matrix\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"Oranges\",\n",
    "    xticklabels=[\"No Churn\", \"Churn\"],\n",
    "    yticklabels=[\"No Churn\", \"Churn\"],\n",
    "    ax=axes[1],\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"Champion Model - Confusion Matrix (Normalized)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].set_ylabel(\"Actual Labels\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Predicted Labels\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display confusion matrix insights\n",
    "tn, fp, fn, tp = cm_champion.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "print(f\"  üìä Confusion Matrix Insights:\")\n",
    "print(f\"    True Negatives:  {tn:,} | False Positives: {fp:,}\")\n",
    "print(f\"    False Negatives: {fn:,} | True Positives:  {tp:,}\")\n",
    "print(f\"    Specificity (TNR): {specificity:.4f}\")\n",
    "print(f\"    Sensitivity (TPR): {sensitivity:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE ROC CURVE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìà Step 7.2: Advanced ROC Curve Analysis\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Calculate ROC curve components\n",
    "fpr_champion, tpr_champion, thresholds_champion = roc_curve(\n",
    "    y_test, champion_pred_probabilities\n",
    ")\n",
    "roc_auc_champion = roc_auc_score(y_test, champion_pred_probabilities)\n",
    "\n",
    "# Create enhanced ROC curve visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot ROC curve with enhanced styling\n",
    "plt.plot(\n",
    "    fpr_champion,\n",
    "    tpr_champion,\n",
    "    color=\"darkblue\",\n",
    "    linewidth=3,\n",
    "    label=f\"Champion Model ROC (AUC = {roc_auc_champion:.4f})\",\n",
    ")\n",
    "\n",
    "# Add comparison baselines\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Random Classifier (AUC = 0.5000)\",\n",
    ")\n",
    "\n",
    "# Enhanced plot formatting\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Champion Model - ROC Curve Analysis\", fontsize=16, fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AUC annotation\n",
    "plt.text(\n",
    "    0.6,\n",
    "    0.2,\n",
    "    f\"AUC Score: {roc_auc_champion:.4f}\",\n",
    "    fontsize=14,\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"),\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# PRECISION-RECALL CURVE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Step 7.3: Precision-Recall Curve Analysis\")\n",
    "print(\"-\" * 44)\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(\n",
    "    y_test, champion_pred_probabilities\n",
    ")\n",
    "avg_precision = precision_score(y_test, champion_predictions)\n",
    "\n",
    "# Create precision-recall visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    recall_vals,\n",
    "    precision_vals,\n",
    "    color=\"darkgreen\",\n",
    "    linewidth=3,\n",
    "    label=f\"Champion Model (Avg Precision = {avg_precision:.4f})\",\n",
    ")\n",
    "\n",
    "# Add baseline\n",
    "baseline_precision = sum(y_test) / len(y_test)\n",
    "plt.axhline(\n",
    "    y=baseline_precision,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Baseline (Random = {baseline_precision:.4f})\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Recall (Sensitivity)\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Precision\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Champion Model - Precision-Recall Curve\", fontsize=16, fontweight=\"bold\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüî¨ Step 7.4: Advanced Feature Importance Analysis\")\n",
    "print(\"-\" * 49)\n",
    "\n",
    "# Extract feature importance from champion model\n",
    "champion_feature_importance = champion_model.named_steps[\n",
    "    \"classifier\"\n",
    "].feature_importances_\n",
    "\n",
    "# Get comprehensive feature names after preprocessing\n",
    "champion_preprocessor = champion_model.named_steps[\"advanced_preprocessing\"]\n",
    "\n",
    "# Extract feature names from each transformer\n",
    "numerical_feature_names = numerical_features\n",
    "categorical_feature_names = list(\n",
    "    champion_preprocessor.named_transformers_[\"categorical_features\"]\n",
    "    .named_steps[\"encoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = numerical_feature_names + categorical_feature_names\n",
    "\n",
    "# Create comprehensive feature importance dataframe\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature\": all_feature_names,\n",
    "        \"Importance\": champion_feature_importance,\n",
    "        \"Feature_Type\": (\n",
    "            [\"Numerical\"] * len(numerical_feature_names)\n",
    "            + [\"Categorical\"] * len(categorical_feature_names)\n",
    "        ),\n",
    "    }\n",
    ").sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(f\"  üèÜ TOP 15 MOST IMPORTANT FEATURES:\")\n",
    "print(f\"  \" + \"=\" * 50)\n",
    "top_features = feature_importance_df.head(15)\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"    {row['Feature']:<30} | {row['Importance']:.6f} | {row['Feature_Type']}\")\n",
    "\n",
    "# Create enhanced feature importance visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot top 20 features with color coding by type\n",
    "top_20_features = feature_importance_df.head(20)\n",
    "colors = [\n",
    "    \"skyblue\" if ft == \"Numerical\" else \"lightcoral\"\n",
    "    for ft in top_20_features[\"Feature_Type\"]\n",
    "]\n",
    "\n",
    "bars = plt.barh(\n",
    "    range(len(top_20_features)), top_20_features[\"Importance\"], color=colors\n",
    ")\n",
    "plt.yticks(range(len(top_20_features)), top_20_features[\"Feature\"])\n",
    "plt.xlabel(\"Feature Importance Score\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\n",
    "    \"Champion Model - Top 20 Feature Importance Analysis\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, importance) in enumerate(zip(bars, top_20_features[\"Importance\"])):\n",
    "    plt.text(\n",
    "        bar.get_width() + 0.001,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{importance:.4f}\",\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"skyblue\", label=\"Numerical Features\"),\n",
    "    Patch(facecolor=\"lightcoral\", label=\"Categorical Features\"),\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc=\"lower right\")\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance summary statistics\n",
    "print(f\"\\n  üìä Feature Importance Summary Statistics:\")\n",
    "print(f\"    Total Features: {len(feature_importance_df):,}\")\n",
    "print(f\"    Mean Importance: {feature_importance_df['Importance'].mean():.6f}\")\n",
    "print(f\"    Std Importance: {feature_importance_df['Importance'].std():.6f}\")\n",
    "print(\n",
    "    f\"    Top 10 Features Cumulative Importance: {feature_importance_df.head(10)['Importance'].sum():.4f}\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚ú® Advanced Visualization & Analytics Suite Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd779d9",
   "metadata": {},
   "source": [
    "## üíæ Step 8: Production-Ready Model Serialization & Deployment Pipeline\n",
    "\n",
    "Implementing comprehensive model persistence strategies with versioning, validation, and deployment-ready artifacts for production environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRODUCTION-READY MODEL SERIALIZATION & DEPLOYMENT PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üíæ Initiating Production-Ready Model Deployment Pipeline...\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE MODEL ARTIFACTS PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüèóÔ∏è Step 8.1: Comprehensive Model Artifacts Preparation\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "# Create deployment directory structure\n",
    "deployment_dir = Path(\"advanced_churn_prediction_deployment\")\n",
    "deployment_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subdirectories for organized deployment\n",
    "(deployment_dir / \"models\").mkdir(exist_ok=True)\n",
    "(deployment_dir / \"metadata\").mkdir(exist_ok=True)\n",
    "(deployment_dir / \"validation\").mkdir(exist_ok=True)\n",
    "(deployment_dir / \"docs\").mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"  üìÅ Deployment Directory Structure Created: {deployment_dir}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED MODEL SERIALIZATION WITH VERSIONING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüíæ Step 8.2: Advanced Model Serialization\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Enhanced model naming with version and timestamp\n",
    "model_version = \"v2.1\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f\"champion_churn_predictor_{model_version}_{timestamp}.joblib\"\n",
    "model_filepath = deployment_dir / \"models\" / model_filename\n",
    "\n",
    "# Serialize champion model with comprehensive metadata\n",
    "model_metadata = {\n",
    "    \"model_name\": \"Advanced_Champion_Churn_Predictor\",\n",
    "    \"model_version\": model_version,\n",
    "    \"creation_timestamp\": datetime.now().isoformat(),\n",
    "    \"training_samples\": len(X_train),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"features_count\": len(all_feature_names),\n",
    "    \"performance_metrics\": champion_metrics,\n",
    "    \"optimal_parameters\": optimal_parameters,\n",
    "    \"cv_score\": optimal_cv_score,\n",
    "    \"feature_names\": all_feature_names,\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"preprocessing_steps\": {\n",
    "        \"numerical\": \"KNN_Imputation ‚Üí Robust_Scaling ‚Üí Standardization\",\n",
    "        \"categorical\": \"Mode_Imputation ‚Üí OneHot_Encoding\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save model with joblib (recommended for scikit-learn)\n",
    "joblib.dump(champion_model, model_filepath, compress=3)\n",
    "print(f\"  ‚úÖ Champion Model Serialized: {model_filename}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata_filepath = (\n",
    "    deployment_dir / \"metadata\" / f\"model_metadata_{model_version}_{timestamp}.json\"\n",
    ")\n",
    "with open(metadata_filepath, \"w\") as f:\n",
    "    import json\n",
    "\n",
    "    json.dump(model_metadata, f, indent=2, default=str)\n",
    "print(f\"  ‚úÖ Model Metadata Saved: {metadata_filepath.name}\")\n",
    "\n",
    "# Additional serialization formats for compatibility\n",
    "pickle_filepath = (\n",
    "    deployment_dir / \"models\" / f\"champion_model_{model_version}_{timestamp}.pkl\"\n",
    ")\n",
    "with open(pickle_filepath, \"wb\") as f:\n",
    "    pickle.dump(champion_model, f)\n",
    "print(f\"  ‚úÖ Pickle Format Saved: {pickle_filepath.name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL VALIDATION & INTEGRITY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüîç Step 8.3: Model Validation & Integrity Check\")\n",
    "print(\"-\" * 47)\n",
    "\n",
    "# Load serialized model for validation\n",
    "loaded_champion_model = joblib.load(model_filepath)\n",
    "print(f\"  ‚úÖ Model Successfully Loaded from: {model_filename}\")\n",
    "\n",
    "# Comprehensive validation tests\n",
    "validation_results = {}\n",
    "\n",
    "# Test 1: Prediction consistency\n",
    "original_predictions = champion_model.predict(X_test[:100])\n",
    "loaded_predictions = loaded_champion_model.predict(X_test[:100])\n",
    "predictions_match = np.array_equal(original_predictions, loaded_predictions)\n",
    "validation_results[\"predictions_consistency\"] = predictions_match\n",
    "\n",
    "# Test 2: Probability consistency\n",
    "original_probabilities = champion_model.predict_proba(X_test[:100])\n",
    "loaded_probabilities = loaded_champion_model.predict_proba(X_test[:100])\n",
    "probabilities_match = np.allclose(\n",
    "    original_probabilities, loaded_probabilities, rtol=1e-10\n",
    ")\n",
    "validation_results[\"probabilities_consistency\"] = probabilities_match\n",
    "\n",
    "# Test 3: Model architecture consistency\n",
    "architecture_match = (\n",
    "    str(type(champion_model)) == str(type(loaded_champion_model))\n",
    "    and champion_model.get_params() == loaded_champion_model.get_params()\n",
    ")\n",
    "validation_results[\"architecture_consistency\"] = architecture_match\n",
    "\n",
    "# Test 4: Performance consistency\n",
    "loaded_accuracy = loaded_champion_model.score(X_test, y_test)\n",
    "performance_match = abs(loaded_accuracy - champion_metrics[\"accuracy\"]) < 1e-10\n",
    "validation_results[\"performance_consistency\"] = performance_match\n",
    "\n",
    "# Display validation results\n",
    "print(f\"  üî¨ Validation Test Results:\")\n",
    "for test_name, result in validation_results.items():\n",
    "    status = \"‚úÖ PASSED\" if result else \"‚ùå FAILED\"\n",
    "    print(f\"    {test_name.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "# Overall validation status\n",
    "all_tests_passed = all(validation_results.values())\n",
    "validation_status = (\n",
    "    \"‚úÖ ALL TESTS PASSED\" if all_tests_passed else \"‚ùå VALIDATION FAILED\"\n",
    ")\n",
    "print(f\"  üèÜ Overall Validation Status: {validation_status}\")\n",
    "\n",
    "# Save validation report\n",
    "validation_report = {\n",
    "    \"validation_timestamp\": datetime.now().isoformat(),\n",
    "    \"model_file\": model_filename,\n",
    "    \"validation_tests\": validation_results,\n",
    "    \"overall_status\": \"PASSED\" if all_tests_passed else \"FAILED\",\n",
    "    \"loaded_model_accuracy\": loaded_accuracy,\n",
    "    \"original_model_accuracy\": champion_metrics[\"accuracy\"],\n",
    "}\n",
    "\n",
    "validation_filepath = (\n",
    "    deployment_dir / \"validation\" / f\"validation_report_{timestamp}.json\"\n",
    ")\n",
    "with open(validation_filepath, \"w\") as f:\n",
    "    json.dump(validation_report, f, indent=2, default=str)\n",
    "print(f\"  üìã Validation Report Saved: {validation_filepath.name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DEPLOYMENT DOCUMENTATION GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìö Step 8.4: Deployment Documentation Generation\")\n",
    "print(\"-\" * 49)\n",
    "\n",
    "# Generate comprehensive deployment documentation\n",
    "deployment_docs = f\"\"\"\n",
    "# Advanced Customer Churn Prediction Model - Deployment Guide\n",
    "\n",
    "## Model Information\n",
    "- **Model Name**: {model_metadata[\"model_name\"]}\n",
    "- **Version**: {model_metadata[\"model_version\"]}\n",
    "- **Created**: {model_metadata[\"creation_timestamp\"]}\n",
    "- **Algorithm**: Random Forest Classifier (Optimized)\n",
    "\n",
    "## Performance Metrics\n",
    "- **Accuracy**: {champion_metrics[\"accuracy\"]:.6f}\n",
    "- **ROC-AUC**: {champion_metrics[\"roc_auc\"]:.6f}\n",
    "- **Precision**: {champion_metrics[\"precision\"]:.6f}\n",
    "- **Recall**: {champion_metrics[\"recall\"]:.6f}\n",
    "- **F1-Score**: {champion_metrics[\"f1_score\"]:.6f}\n",
    "\n",
    "## Model Features\n",
    "- **Total Features**: {len(all_feature_names)}\n",
    "- **Numerical Features**: {len(numerical_features)}\n",
    "- **Categorical Features**: {len(categorical_features)}\n",
    "\n",
    "## Usage Example\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load('{model_filename}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_data)\n",
    "probabilities = model.predict_proba(new_data)\n",
    "```\n",
    "\n",
    "## Deployment Notes\n",
    "- Model requires the same preprocessing pipeline used during training\n",
    "- Input data must contain all {len(all_feature_names)} features\n",
    "- Model is production-ready and validated\n",
    "\"\"\"\n",
    "\n",
    "docs_filepath = deployment_dir / \"docs\" / f\"deployment_guide_{model_version}.md\"\n",
    "with open(docs_filepath, \"w\") as f:\n",
    "    f.write(deployment_docs)\n",
    "print(f\"  üìñ Deployment Guide Created: {docs_filepath.name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DEPLOYMENT SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüéØ Step 8.5: Deployment Pipeline Summary\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "file_sizes = {\n",
    "    \"Joblib Model\": model_filepath.stat().st_size / (1024 * 1024),\n",
    "    \"Pickle Model\": pickle_filepath.stat().st_size / (1024 * 1024),\n",
    "    \"Metadata\": metadata_filepath.stat().st_size / 1024,\n",
    "    \"Validation Report\": validation_filepath.stat().st_size / 1024,\n",
    "}\n",
    "\n",
    "print(f\"  üìä Deployment Artifacts Summary:\")\n",
    "print(f\"    Deployment Directory: {deployment_dir}\")\n",
    "print(f\"    Model Files Created: 2 (joblib + pickle)\")\n",
    "print(f\"    Metadata Files: 1\")\n",
    "print(f\"    Validation Files: 1\")\n",
    "print(f\"    Documentation Files: 1\")\n",
    "print(f\"\\n  üíæ File Sizes:\")\n",
    "for filename, size in file_sizes.items():\n",
    "    unit = \"MB\" if filename.endswith(\"Model\") else \"KB\"\n",
    "    print(f\"    {filename}: {size:.2f} {unit}\")\n",
    "\n",
    "print(f\"\\nüöÄ Production-Ready Model Deployment Pipeline Complete!\")\n",
    "print(f\"üéØ Model Ready for Production Deployment\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf4cb9",
   "metadata": {},
   "source": [
    "## üåê Step 9: Interactive Web Application Development\n",
    "\n",
    "Creating a sophisticated Streamlit web application for real-time churn prediction with advanced user interface, comprehensive input validation, and detailed prediction analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED INTERACTIVE WEB APPLICATION DEVELOPMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üåê Creating Advanced Interactive Web Application...\")\n",
    "\n",
    "# =============================================================================\n",
    "# STREAMLIT APPLICATION CODE GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüíª Step 9.1: Advanced Streamlit Application Development\")\n",
    "print(\"-\" * 56)\n",
    "\n",
    "# Enhanced Streamlit application code with advanced features\n",
    "streamlit_app_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED STREAMLIT APPLICATION CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Enhanced page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"üéØ Advanced Customer Churn Prediction System\",\n",
    "    page_icon=\"üìä\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS for enhanced styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        font-size: 3rem;\n",
    "        color: #1f77b4;\n",
    "        text-align: center;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .metric-card {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        border: 1px solid #e0e0e0;\n",
    "    }\n",
    "    .prediction-high-risk {\n",
    "        background-color: #ffebee;\n",
    "        border-left: 5px solid #f44336;\n",
    "        padding: 1rem;\n",
    "    }\n",
    "    .prediction-low-risk {\n",
    "        background-color: #e8f5e8;\n",
    "        border-left: 5px solid #4caf50;\n",
    "        padding: 1rem;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# =============================================================================\n",
    "# APPLICATION HEADER & INTRODUCTION\n",
    "# =============================================================================\n",
    "\n",
    "st.markdown('<h1 class=\"main-header\">üéØ Advanced Customer Churn Prediction System</h1>', \n",
    "            unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "<div style=\"text-align: center; margin-bottom: 2rem;\">\n",
    "    <p style=\"font-size: 1.2rem; color: #666;\">\n",
    "        AI-Powered Customer Retention Analytics Platform<br>\n",
    "        <em>Predicting churn likelihood with advanced machine learning algorithms</em>\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL LOADING & CACHING\n",
    "# =============================================================================\n",
    "\n",
    "@st.cache_resource\n",
    "def load_advanced_model():\n",
    "    \"\"\"Load the trained model with error handling\"\"\"\n",
    "    try:\n",
    "        # Try to load the latest model\n",
    "        model_path = \"advanced_churn_prediction_deployment/models/\"\n",
    "        model_files = list(Path(model_path).glob(\"champion_churn_predictor_*.joblib\"))\n",
    "        \n",
    "        if model_files:\n",
    "            latest_model = max(model_files, key=lambda x: x.stat().st_mtime)\n",
    "            model = joblib.load(latest_model)\n",
    "            st.success(f\"‚úÖ Model loaded successfully: {latest_model.name}\")\n",
    "            return model, latest_model.name\n",
    "        else:\n",
    "            st.error(\"‚ùå No model files found. Please ensure the model is trained and saved.\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ùå Error loading model: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "@st.cache_data\n",
    "def load_model_metadata():\n",
    "    \"\"\"Load model metadata for display\"\"\"\n",
    "    try:\n",
    "        metadata_path = \"advanced_churn_prediction_deployment/metadata/\"\n",
    "        metadata_files = list(Path(metadata_path).glob(\"model_metadata_*.json\"))\n",
    "        \n",
    "        if metadata_files:\n",
    "            latest_metadata = max(metadata_files, key=lambda x: x.stat().st_mtime)\n",
    "            with open(latest_metadata, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            return metadata\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Load model and metadata\n",
    "model, model_name = load_advanced_model()\n",
    "metadata = load_model_metadata()\n",
    "\n",
    "if model is None:\n",
    "    st.stop()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL INFORMATION DASHBOARD\n",
    "# =============================================================================\n",
    "\n",
    "if metadata:\n",
    "    st.markdown(\"## üìä Model Information Dashboard\")\n",
    "    \n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    \n",
    "    with col1:\n",
    "        st.metric(\"Model Version\", metadata.get('model_version', 'N/A'))\n",
    "    with col2:\n",
    "        st.metric(\"Training Samples\", f\"{metadata.get('training_samples', 0):,}\")\n",
    "    with col3:\n",
    "        st.metric(\"ROC-AUC Score\", f\"{metadata.get('performance_metrics', {}).get('roc_auc', 0):.4f}\")\n",
    "    with col4:\n",
    "        st.metric(\"Accuracy\", f\"{metadata.get('performance_metrics', {}).get('accuracy', 0):.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED INPUT INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "st.markdown(\"## üîß Customer Profile Configuration\")\n",
    "\n",
    "# Create two main columns for input\n",
    "input_col1, input_col2 = st.columns(2)\n",
    "\n",
    "with input_col1:\n",
    "    st.markdown(\"### üìä Account Information\")\n",
    "    \n",
    "    # Numerical inputs with enhanced controls\n",
    "    tenure = st.slider(\"üìÖ Tenure (months)\", 0, 100, 24, \n",
    "                      help=\"Number of months the customer has been with the company\")\n",
    "    \n",
    "    monthly_charges = st.slider(\"üí∞ Monthly Charges ($)\", 0.0, 200.0, 65.0, 0.01,\n",
    "                               help=\"Customer's average monthly charges\")\n",
    "    \n",
    "    total_charges = st.slider(\"üí≥ Total Charges ($)\", 0.0, 10000.0, 2000.0, 10.0,\n",
    "                             help=\"Customer's total charges to date\")\n",
    "\n",
    "with input_col2:\n",
    "    st.markdown(\"### üè∑Ô∏è Service Configuration\")\n",
    "    \n",
    "    # Categorical inputs with enhanced options\n",
    "    contract = st.selectbox(\"üìã Contract Type\", \n",
    "                           [\"Month-to-month\", \"One year\", \"Two year\"],\n",
    "                           help=\"Type of contract the customer has\")\n",
    "    \n",
    "    internet_service = st.selectbox(\"üåê Internet Service\", \n",
    "                                   [\"DSL\", \"Fiber optic\", \"No\"],\n",
    "                                   help=\"Type of internet service\")\n",
    "    \n",
    "    payment_method = st.selectbox(\"üí≥ Payment Method\", [\n",
    "        \"Electronic check\", \"Mailed check\", \n",
    "        \"Bank transfer (automatic)\", \"Credit card (automatic)\"\n",
    "    ], help=\"Customer's preferred payment method\")\n",
    "\n",
    "# Additional service options in expandable sections\n",
    "with st.expander(\"üîß Advanced Service Options\"):\n",
    "    col_adv1, col_adv2 = st.columns(2)\n",
    "    \n",
    "    with col_adv1:\n",
    "        online_security = st.selectbox(\"üîí Online Security\", \n",
    "                                      [\"No\", \"Yes\", \"No internet service\"])\n",
    "        tech_support = st.selectbox(\"üõ†Ô∏è Tech Support\", \n",
    "                                   [\"No\", \"Yes\", \"No internet service\"])\n",
    "        online_backup = st.selectbox(\"üíæ Online Backup\", \n",
    "                                    [\"No\", \"Yes\", \"No internet service\"])\n",
    "    \n",
    "    with col_adv2:\n",
    "        device_protection = st.selectbox(\"üì± Device Protection\", \n",
    "                                        [\"No\", \"Yes\", \"No internet service\"])\n",
    "        streaming_tv = st.selectbox(\"üì∫ Streaming TV\", \n",
    "                                   [\"No\", \"Yes\", \"No internet service\"])\n",
    "        streaming_movies = st.selectbox(\"üé¨ Streaming Movies\", \n",
    "                                       [\"No\", \"Yes\", \"No internet service\"])\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED PREDICTION ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_advanced_input_data():\n",
    "    \"\"\"Prepare input data with all required features\"\"\"\n",
    "    \n",
    "    # Base customer data\n",
    "    customer_profile = {\n",
    "        'tenure': tenure,\n",
    "        'MonthlyCharges': monthly_charges,\n",
    "        'TotalCharges': total_charges,\n",
    "        'Contract': contract,\n",
    "        'InternetService': internet_service,\n",
    "        'PaymentMethod': payment_method,\n",
    "        'OnlineSecurity': online_security,\n",
    "        'TechSupport': tech_support,\n",
    "        'OnlineBackup': online_backup,\n",
    "        'DeviceProtection': device_protection,\n",
    "        'StreamingTV': streaming_tv,\n",
    "        'StreamingMovies': streaming_movies\n",
    "    }\n",
    "    \n",
    "    # Enhanced default values for comprehensive model compatibility\n",
    "    default_customer_attributes = {\n",
    "        'gender': 'Female',\n",
    "        'SeniorCitizen': 0,\n",
    "        'Partner': 'No',\n",
    "        'Dependents': 'No',\n",
    "        'PhoneService': 'Yes',\n",
    "        'MultipleLines': 'No',\n",
    "        'PaperlessBilling': 'Yes'\n",
    "    }\n",
    "    \n",
    "    # Merge all customer data\n",
    "    complete_profile = {**customer_profile, **default_customer_attributes}\n",
    "    \n",
    "    # Advanced feature engineering (matching training pipeline)\n",
    "    complete_profile['AvgChargesPerMonth'] = complete_profile['TotalCharges'] / (complete_profile['tenure'] + 1)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame([complete_profile])\n",
    "\n",
    "# =============================================================================\n",
    "# PREDICTION INTERFACE & RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "st.markdown(\"## üéØ Churn Prediction Analysis\")\n",
    "\n",
    "# Enhanced prediction button\n",
    "if st.button(\"üöÄ Generate Advanced Churn Prediction\", type=\"primary\", use_container_width=True):\n",
    "    \n",
    "    # Prepare customer data\n",
    "    customer_input = prepare_advanced_input_data()\n",
    "    \n",
    "    # Display customer profile\n",
    "    with st.expander(\"üë§ Customer Profile Summary\", expanded=True):\n",
    "        st.dataframe(customer_input.style.highlight_max(axis=0), use_container_width=True)\n",
    "    \n",
    "    # Make predictions with comprehensive analysis\n",
    "    with st.spinner(\"ü§ñ AI Model Processing Customer Profile...\"):\n",
    "        try:\n",
    "            # Generate predictions\n",
    "            prediction = model.predict(customer_input)[0]\n",
    "            prediction_proba = model.predict_proba(customer_input)[0]\n",
    "            \n",
    "            # Calculate risk metrics\n",
    "            churn_probability = prediction_proba[1]\n",
    "            confidence_score = max(prediction_proba)\n",
    "            risk_level = \"HIGH RISK\" if churn_probability > 0.6 else \"MODERATE RISK\" if churn_probability > 0.3 else \"LOW RISK\"\n",
    "            \n",
    "            # Enhanced results display\n",
    "            st.markdown(\"### üéØ Prediction Results\")\n",
    "            \n",
    "            # Main prediction result\n",
    "            prediction_text = \"‚ö†Ô∏è LIKELY TO CHURN\" if prediction == 1 else \"‚úÖ LIKELY TO STAY\"\n",
    "            risk_color = \"prediction-high-risk\" if prediction == 1 else \"prediction-low-risk\"\n",
    "            \n",
    "            st.markdown(f'''\n",
    "            <div class=\"{risk_color}\">\n",
    "                <h3>{prediction_text}</h3>\n",
    "                <p><strong>Risk Level:</strong> {risk_level}</p>\n",
    "                <p><strong>Confidence:</strong> {confidence_score * 100:.1f}%</p>\n",
    "            </div>\n",
    "            ''', unsafe_allow_html=True)\n",
    "            \n",
    "            # Detailed metrics\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            \n",
    "            with col1:\n",
    "                st.metric(\"Churn Probability\", f\"{churn_probability * 100:.1f}%\",\n",
    "                         delta=f\"{churn_probability * 100 - 26.5:.1f}% vs avg\")\n",
    "            \n",
    "            with col2:\n",
    "                st.metric(\"Retention Probability\", f\"{(1-churn_probability) * 100:.1f}%\")\n",
    "            \n",
    "            with col3:\n",
    "                st.metric(\"Prediction Confidence\", f\"{confidence_score * 100:.1f}%\")\n",
    "            \n",
    "            # Probability distribution visualization\n",
    "            st.markdown(\"### üìä Probability Distribution\")\n",
    "            \n",
    "            # Create probability chart\n",
    "            prob_data = pd.DataFrame({\n",
    "                'Outcome': ['Stay', 'Churn'],\n",
    "                'Probability': [prediction_proba[0], prediction_proba[1]],\n",
    "                'Color': ['#4CAF50', '#F44336']\n",
    "            })\n",
    "            \n",
    "            fig = px.bar(prob_data, x='Outcome', y='Probability', color='Color',\n",
    "                        color_discrete_map={'#4CAF50': '#4CAF50', '#F44336': '#F44336'})\n",
    "            fig.update_layout(showlegend=False, height=400)\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Risk factors analysis\n",
    "            st.markdown(\"### üîç Risk Factors Analysis\")\n",
    "            \n",
    "            risk_factors = []\n",
    "            if monthly_charges > 80:\n",
    "                risk_factors.append(\"High monthly charges\")\n",
    "            if contract == \"Month-to-month\":\n",
    "                risk_factors.append(\"Month-to-month contract\")\n",
    "            if payment_method == \"Electronic check\":\n",
    "                risk_factors.append(\"Electronic check payment\")\n",
    "            if internet_service == \"Fiber optic\" and online_security == \"No\":\n",
    "                risk_factors.append(\"Fiber optic service without security\")\n",
    "            \n",
    "            if risk_factors:\n",
    "                st.warning(\"‚ö†Ô∏è Identified Risk Factors:\")\n",
    "                for factor in risk_factors:\n",
    "                    st.write(f\"‚Ä¢ {factor}\")\n",
    "            else:\n",
    "                st.success(\"‚úÖ No major risk factors identified\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Prediction Error: {str(e)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# APPLICATION SIDEBAR & INFORMATION\n",
    "# =============================================================================\n",
    "\n",
    "with st.sidebar:\n",
    "    st.markdown(\"## üìö Model Information\")\n",
    "    \n",
    "    if metadata:\n",
    "        st.json({\n",
    "            \"Model Version\": metadata.get('model_version', 'N/A'),\n",
    "            \"Creation Date\": metadata.get('creation_timestamp', 'N/A')[:10],\n",
    "            \"Performance Metrics\": {\n",
    "                \"Accuracy\": f\"{metadata.get('performance_metrics', {}).get('accuracy', 0):.4f}\",\n",
    "                \"ROC-AUC\": f\"{metadata.get('performance_metrics', {}).get('roc_auc', 0):.4f}\",\n",
    "                \"F1-Score\": f\"{metadata.get('performance_metrics', {}).get('f1_score', 0):.4f}\"\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    st.markdown(\"## üîß How to Use\")\n",
    "    st.info(\"\"\"\n",
    "    1. **Configure Customer Profile**: Adjust the sliders and dropdowns to match the customer's profile\n",
    "    2. **Generate Prediction**: Click the prediction button to analyze churn risk\n",
    "    3. **Review Results**: Examine the probability scores and risk factors\n",
    "    4. **Take Action**: Use insights for retention strategies\n",
    "    \"\"\")\n",
    "    \n",
    "    st.markdown(\"## ‚ö° Features\")\n",
    "    st.success(\"\"\"\n",
    "    ‚Ä¢ Real-time churn prediction\n",
    "    ‚Ä¢ Advanced risk factor analysis\n",
    "    ‚Ä¢ Interactive probability visualization\n",
    "    ‚Ä¢ Comprehensive customer profiling\n",
    "    ‚Ä¢ Production-ready ML model\n",
    "    \"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FOOTER\n",
    "# =============================================================================\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"\"\"\n",
    "<div style=\"text-align: center; color: #666; margin-top: 2rem;\">\n",
    "    <p>üéØ Advanced Customer Churn Prediction System | \n",
    "    Powered by Machine Learning & Streamlit | \n",
    "    Version 2.1</p>\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "'''\n",
    "\n",
    "# Save the enhanced Streamlit application\n",
    "app_filepath = deployment_dir / \"advanced_churn_prediction_app.py\"\n",
    "with open(app_filepath, 'w', encoding='utf-8') as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(f\"  ‚úÖ Advanced Streamlit Application Created: {app_filepath.name}\")\n",
    "print(f\"  üåê Application Features:\")\n",
    "print(f\"    ‚Ä¢ Enhanced UI with custom CSS styling\")\n",
    "print(f\"    ‚Ä¢ Advanced customer profiling interface\")  \n",
    "print(f\"    ‚Ä¢ Real-time prediction with confidence scores\")\n",
    "print(f\"    ‚Ä¢ Interactive probability visualization\")\n",
    "print(f\"    ‚Ä¢ Comprehensive risk factor analysis\")\n",
    "print(f\"    ‚Ä¢ Production-ready model integration\")\n",
    "\n",
    "# =============================================================================\n",
    "# APPLICATION LAUNCH INSTRUCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüöÄ Step 9.2: Application Deployment Instructions\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "launch_instructions = f\"\"\"\n",
    "# üåê Advanced Churn Prediction App - Launch Instructions\n",
    "\n",
    "## Quick Start\n",
    "```bash\n",
    "# Navigate to deployment directory\n",
    "cd {deployment_dir}\n",
    "\n",
    "# Install required packages\n",
    "pip install streamlit plotly\n",
    "\n",
    "# Launch the application\n",
    "streamlit run advanced_churn_prediction_app.py\n",
    "```\n",
    "\n",
    "## Application URL\n",
    "After launching, the app will be available at:\n",
    "- **Local URL**: http://localhost:8501\n",
    "- **Network URL**: http://[your-ip]:8501\n",
    "\n",
    "## Features Overview\n",
    "- üéØ Real-time churn prediction\n",
    "- üìä Interactive visualizations\n",
    "- üîç Advanced risk analysis\n",
    "- üë§ Comprehensive customer profiling\n",
    "- üìà Performance metrics dashboard\n",
    "\n",
    "## Production Deployment\n",
    "For production deployment, consider:\n",
    "- Docker containerization\n",
    "- Cloud hosting (AWS, GCP, Azure)\n",
    "- Load balancing for high traffic\n",
    "- SSL certificate for HTTPS\n",
    "\"\"\"\n",
    "\n",
    "instructions_filepath = deployment_dir / \"docs\" / \"app_launch_instructions.md\"\n",
    "with open(instructions_filepath, 'w') as f:\n",
    "    f.write(launch_instructions)\n",
    "\n",
    "print(f\"  üìã Launch Instructions Created: {instructions_filepath.name}\")\n",
    "print(f\"  üéØ Ready for Application Deployment!\")\n",
    "\n",
    "print(f\"\\n‚ú® Advanced Interactive Web Application Development Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08517078",
   "metadata": {},
   "source": [
    "## üéâ PROJECT COMPLETION & SUMMARY\n",
    "\n",
    "### üèÜ Advanced Customer Churn Prediction System - Complete Implementation\n",
    "\n",
    "This comprehensive machine learning project successfully demonstrates a production-ready customer churn prediction system with the following achievements:\n",
    "\n",
    "#### ‚úÖ **Key Accomplishments:**\n",
    "\n",
    "- **Advanced Data Engineering**: Sophisticated preprocessing with KNN imputation and robust scaling\n",
    "- **Multi-Algorithm Comparison**: Comprehensive evaluation of Logistic Regression, Random Forest, and Gradient Boosting\n",
    "- **Hyperparameter Optimization**: Advanced grid search with stratified cross-validation\n",
    "- **Feature Engineering**: Intelligent feature creation and importance analysis\n",
    "- **Production Deployment**: Complete model serialization with versioning and validation\n",
    "- **Interactive Web App**: Professional Streamlit application with real-time predictions\n",
    "\n",
    "#### üìä **Final Model Performance:**\n",
    "\n",
    "- **Algorithm**: Optimized Random Forest Classifier\n",
    "- **ROC-AUC Score**: ~0.85+ (depending on hyperparameter optimization results)\n",
    "- **Features**: 20+ engineered features with comprehensive preprocessing\n",
    "- **Validation**: 5-fold stratified cross-validation with robust performance metrics\n",
    "\n",
    "#### üöÄ **Deployment Ready:**\n",
    "\n",
    "- Production-ready model artifacts with comprehensive metadata\n",
    "- Interactive web application for real-time predictions\n",
    "- Complete documentation and validation reports\n",
    "- Scalable architecture for enterprise deployment\n",
    "\n",
    "#### üí° **Advanced Features Implemented:**\n",
    "\n",
    "- Intelligent missing value handling with KNN imputation\n",
    "- Robust feature scaling and encoding strategies\n",
    "- Advanced visualization suite with confusion matrices and ROC curves\n",
    "- Comprehensive model validation and integrity checks\n",
    "- Professional-grade deployment pipeline with versioning\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Next Steps for Production:**\n",
    "\n",
    "1. **Model Monitoring**: Implement drift detection and performance monitoring\n",
    "2. **A/B Testing**: Set up experiments for model performance comparison\n",
    "3. **API Development**: Create REST API endpoints for model serving\n",
    "4. **Container Deployment**: Docker containerization for cloud deployment\n",
    "5. **Continuous Integration**: Set up automated retraining pipelines\n",
    "\n",
    "---\n",
    "\n",
    "**üèÖ This project showcases enterprise-level machine learning practices with production-ready implementation standards.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811901f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
